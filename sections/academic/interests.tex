My current research focues on 3 areas of Mechanistic Interpretability for Large Language Models (LLMs):
\begin{zitemize}
    \item \textbf{Activation Space Geometry (1):} Understanding how concepts and behaviors are represented
in the activation space of LLMs. To what extent the Linear Representation Hypothesis and
the Platonic Representation Hypothesis hold?~\cite{vu2025angular}
    \item \textbf{Application of Mechanistic Interpretability (2):} Activation Steering,
Model Editing, Model Fingerprinting, and other applications.~\cite{vu2025angular,lee2025momentum,nguyen2025activationsteeringfeedbackcontroller}
    \item \textbf{Modular Decomposition (3):} Decomposing LLMs into smaller, interpretable modules that
    perform specific functions, and studying their interactions.~\cite{vu2025angular,son2022jointly,nguyen2021span}
\end{zitemize}